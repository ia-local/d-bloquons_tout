// Fichier : services/ai.js (INTEGRATION DU PROMPT ENGINE ET VISION AI)

const Groq = require('groq-sdk');
const { GoogleGenerativeAI } = require('@google/generative-ai');

// üõë Importation du Prompt Engine (Gestion de la structure des messages)
const promptEngine = require('./promptEngine'); 

// üõë UNE SEULE IMPORTATION DE CONFIGURATION EST N√âCESSAIRE :
const { GROQ_API_KEY, GEMINI_API_KEY, GROQ_MODEL, AI_PERSONAS, CATEGORIES_TO_CLASSIFY } = require('../config');

// Assurez-vous que services/utils.js contient bien la fonction cosineSimilarity
// const { cosineSimilarity } = require('./utils'); 

// üõë SIMULATION DE L'IMPORTATION DES CONSTANTES DE MOD√àLE (pour r√©f√©rence)
const AI_MODELS = {
    DEFAULT: 'llama-3.1-8b-instant',
    AVOCAT: 'deepseek-r1-distill-llama-70b', 
    VISION: 'meta-llama/llama-4-scout-17b-16e-instruct' 
};


// --- 1. INITIALISATION DES CLIENTS ET GESTION DES CL√âS ---

// Initialisation du client Groq 
const groq = GROQ_API_KEY 
    ? new Groq({ apiKey: GROQ_API_KEY }) 
    // Fallback: Objet simul√©
    : { chat: { completions: { create: async () => ({ choices: [{ message: { content: "Erreur: Cl√© Groq manquante." } }] }) } } }; 

const genAI = GEMINI_API_KEY ? new GoogleGenerativeAI(GEMINI_API_KEY) : null;

// Variables pour les embeddings
let categoryEmbeddings = [];


// --- 2. FONCTIONS PUBLIQUES ---

/**
 * Obtient une r√©ponse de l'API Groq (requ√™te textuelle).
 */
async function getGroqChatResponse(promptInput, model, systemMessageContent) {
    if (!GROQ_API_KEY) { 
        console.error("‚ùå Erreur: Tentative d'appel Groq sans cl√© API.");
        return (await groq.chat.completions.create({})).choices[0].message.content; 
    }

    try {
        // üõë Utilisation du Prompt Engine pour structurer les messages
        const messages = promptEngine.generateMessages(systemMessageContent, promptInput);
        
        const chatCompletion = await groq.chat.completions.create({ 
            messages: messages, 
            model: model, 
            temperature: 0.7, 
            max_tokens: 2048 
        });
        
        return chatCompletion.choices[0].message.content;
    } catch (error) {
        console.error(`‚ùå Erreur lors de la g√©n√©ration de la r√©ponse IA (Groq model: ${model}):`, error);
        throw new Error('√âchec lors du traitement de la demande Groq.');
    }
}

/**
 * Obtient une r√©ponse de l'API Groq pour une requ√™te multi-modale (texte et image).
 */
async function getVisionAnalysis(textPrompt, base64Image, model, systemMessageContent) {
    if (!GROQ_API_KEY) { 
        throw new Error("‚ùå Cl√© Groq manquante pour l'analyse visuelle.");
    }

    try {
        const messages = [];
        
        // üõë 1. Message Syst√®me (Auto-compl√©tion du r√¥le)
        if (systemMessageContent) {
            messages.push(promptEngine.buildMessage(promptEngine.ROLES.SYSTEM, systemMessageContent));
        }
        
        // üõë 2. Message Utilisateur Multi-modal
        const userMessage = {
            "role": promptEngine.ROLES.USER,
            "content": [
                { "type": "text", "text": textPrompt },
                {
                    "type": "image_url",
                    "image_url": { "url": `data:image/jpeg;base64,${base64Image}` }
                }
            ]
        };
        messages.push(userMessage);

        const chatCompletion = await groq.chat.completions.create({
            messages: messages, // Tableau de messages structur√©
            model: model, 
            temperature: 0.7,
            max_tokens: 1024
        });
        
        return chatCompletion.choices[0].message.content;
    } catch (error) {
        console.error(`‚ùå Erreur lors de l'analyse visuelle (Groq model: ${model}):`, error);
        throw new Error('√âchec lors du traitement de la demande Groq Vision AI.');
    }
}


/**
 * G√©n√®re les embeddings pour les cat√©gories de classification en utilisant Gemini.
 */
async function generateCategoryEmbeddings() {
    if (!genAI) {
        console.error("‚ö†Ô∏è Gemini non disponible. Classification IA par embedding d√©sactiv√©e.");
        categoryEmbeddings = [];
        return;
    }
    
    try {
        // ... (Logique de g√©n√©ration d'embeddings inchang√©e) ...
        console.log("üì° Tentative de g√©n√©ration des embeddings pour les cat√©gories via Gemini...");
        
        const embeddingModel = genAI.getGenerativeModel({ model: "embedding-001" });
        
        const results = await Promise.all(
            CATEGORIES_TO_CLASSIFY.map(cat => 
                embeddingModel.embedContent({ 
                    content: { parts: [{ text: cat.text }] }
                })
            )
        );
        
        categoryEmbeddings = results.map((res, i) => ({
            name: CATEGORIES_TO_CLASSIFY[i].name,
            embedding: res.embedding.values
        }));
        
        console.log("üóûÔ∏è Embeddings des cat√©gories g√©n√©r√©s et stock√©s.");
        
    } catch (error) {
        console.error("‚ö†Ô∏è √âchec critique de la g√©n√©ration des embeddings IA (Gemini).", error.message);
        categoryEmbeddings = [];
    }
}

// --- 3. EXPORTS DU MODULE ---
module.exports = {
    // Clients
    groq,
    genAI,
    
    // Constantes et √âtat
    GROQ_MODEL,
    AI_PERSONAS,
    categoryEmbeddings,
    
    // Fonctions
    getGroqChatResponse,
    getVisionAnalysis, // üõë NOUVELLE FONCTION EXPORT√âE
    generateCategoryEmbeddings,
    
    // Utilitaires
    AI_MODELS // Exporter les mod√®les pour √™tre utilis√©s dans les routes
};